\section{Experimental Results}\label{sec:exp}
In this section we briefly describe our experimental setup, then provide some performance data on the JIT compiler itself, i.e. how fast it compiles, and finally we compare the code produced from the JIT compiler with the conventionally optimized code.

\mypar{Experimental setup}
All measurements were performed on a Intel Core i7-3632QM (Ivy Bridge) processor running at a clock frequency of 3.2 GHz with cache sizes of 32kB (L1 data), 256kB (L2) and 6MB (L3). The clang compiler version 3.5 was used for all measurements with optimization flags: \texttt{-O3 -march=native}.

For the particle system itself we have seen a nearly non-existing influence from the number of particles. When going from a small particle set that fits into the first level cache to a multi GB set the effect on the overall runtime is around one or two additional cpu cycles per particle and therefore can simply be ignored for any practical purpose. This behaviour is explained due to the perfect sequential traversal over the particles and the high enough computational intensity.
This provides a perfect data access pattern for the hardware prefetcher, masking the memory accesses.
%~ Specify the platform (processor, frequency, cache sizes)
%~ as well as the compiler, version, and flags used. I strongly recommend that you play with optimization flags and consider also icc for additional potential speedup.


\mypar{JIT Compiler}
\begin{figure}[t]\centering
  \includegraphics[width=0.5\textwidth]{jit_perf.pdf}
  \caption{Performance of the JIT compiler itself
  \label{perf_jit}}
\end{figure}
First we consider the performance of the JIT compiler itself. While compilation speed was not our primary concern, we still have to make sure it is suited for real time usage. Meaning we want compilation times to be at least in the low milisecond range. For reference, the time budget to calculate a single update cycle in real time graphics tasks is typically 8-30 miliseconds.

If we take a look at where the jit compiler spends its time (fig~\ref{perf_jit}) we immediately see that all except two stages run in the range of a few microseconds and only the scheduling and code generation take significantly more time. \\
The code generation runs in approximately constant time, around 40 to 120 microseconds, where most of the time is used for two system-calls. These two calls, \texttt{mmap} and \texttt{mprotect}, cause page faults and context switches within the operating system and therefore impose a constant cost that is hard to avoid. The actual writing of the code is only a minor part of the generation stage. \\
Turning our attention to the scheduling stage, we immediately see that the time used for the scheduling steps is strongly dependant on the number of rules, and by extension the number of instructions that are composed together. 
For scenarios with a few rules, the scheduling is eclipsed by the code generation, but for more complex scenarios scheduling takes more time.
Still, the whole JIT compiler runs in the sub-millisecond range for reasonable use cases and is therefore fast enough for real time usage.

\mypar{Code Perfomance}
When looking at the performance of the code itself, we distinguish two situations. The first one encompasses scenarios were there is only one or two rules, which allows us to see how well our JIT compiler performs in direct comparison with AOT compiler. The second scenario takes a closer look at simulations with at least four and up to more than twenty rules, exhibiting the potential for optimizations across multiple rules. The results in the plot represent the following configurations:
\begin{description}
\item[naive:] the naive implementation in SSA form
\item[vector SSE:] implemented 3D-vector math in SSE instructions
\item[parallel SSE 1:] parallel processing of four particles without loop unrolling
\item[parallel SSE 8:] parallel processing of four particles with 8-fold loop unrolling
\item[JIT scalar:] the naive implementation as produced by the JIT compiler by emitting scalar operations only
\item[JIT SSE:] vectorized code form the JIT compiler using 128bit vector register, i.e processing 4 particles in parallel
\item[JIT AVX:] vectorized code from the JIT compiler using the new 256bit register introduced with intel AVX extensions
\end{description}

\begin{figure}[th]\centering
  \includegraphics[scale=0.6]{single_dual_rules.pdf}
  \caption{Performance comparison for single or dual rule scenarios
  \label{perf_single}}
\end{figure}

Looking at figure~\ref{perf_single} we can see that both our JIT compiled and hand optimized code outperform the naive implementation to varying degrees. Due to the required data shuffling neither can fully realize the theoretical speed-up from vector instructions. In general we see the JIT compiled code to be at least equally as fast as the conventional code. Also note that going from 4-way to 8-way vector instructions does not yield significant speed ups and in some cases even results in slow downs. This may be in part caused by added data shuffling. On the used processor, some instructions (\texttt{sqrtps} and \texttt{divps} specifically) also have twice the latency and half the throughput in their 256bit versions. Therefore they do not provide an actual advantage over their 128bit counterparts. The test cases called \emph{central4} and \emph{central8} in figure \ref{perf_multi} are strongly affected by this, since their running time is dominated by those two instructions.

\begin{figure}[th]\centering
  \includegraphics[scale=0.6]{multi_rules.pdf}
  \caption{Performance comparison for multi rule scenarios
  \label{perf_multi}}
\end{figure}

The test cases shown in figure~\ref{perf_multi} use more rules at once and we see the results shift in favour of both optimization paths, with an advantage for the JIT compiler. For both the JIT compiled and conventional code the presence of many rules and therefore many operations allows to compensate the data shuffling overhead we have seen in the previous comparison. Looking closely at the results we can see that the \texttt{jit avx} code always outperforms every other version, and the \texttt{jit sse} code outperfoming the equivalent hand optimized code, \texttt{parallel sse 1}, as well. Comparing the two conventionally optimized code versions \texttt{parallel sse 1} and \texttt{parallel sse 8} indicates that loop unrolling still increases instruction level parallelism, since the version without loop unrolling is still latency limited.

One of the main advantages of the JIT compiler is that it significantly reduces the amount of actually generated instructions. The optimization passes eliminate up to half of the instructions present in the input rules. Due to the types of optimizations that are performed it does however not reduce the critical path length. So the total latency of one loop iteration is unaffected when assuming perfect instruction level paralellism. This reinforces the observation that loop unrolling could provide a speed up for the JIT compiled code by allowing more efficient interleaving of independent paths of execution.



