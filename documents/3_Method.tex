\section{Implementation Details}\label{sec:method}

Starting from a simple, straight-forward baseline implementation we have introduced two main paths of optimization which we discuss in detail in this section. On one hand we applied conventional optimizations, e.g. vectorization, to the baseline implementation. On the other hand we have the JIT compiler itself, which implements all the necessary steps to produce executable code.

\mypar{Baseline Implementation}
Our baseline implementation consists of all the possible rules, each accessible through a function pointer, and an array of particles. In order to run the simulation one can now simply collect a list of desired rules and pass them to the main simulation function. This function then simply iterates over all particles and applies every rule to each particle.

\mypar{Conventional Optimizations}
The dynamic nature of the rule composition limits a conventional compiler or the programmer to optimizing the code on a per rule basis. With the baseline code already written in SSA form (single static assignment) we have then implemented two variants of vectorization. The first one implementing 3D-vector arithmetic in SSE code and the second one processing multiple particles in parallel.

Implementing 3D-vector arithmetic may seem attractive but poses two problems. SSE registers can hold four floats (single precision floating point number) in 128 bits which means for 3D-vectors we already waste 1/4th of the register space and going to the newer AVX instruction at 256 bit wide registers means either wasting 5/8th of the available space or rewriting all the code to process two 3D-vectors in parallel. Furthermore there are no dedicated instructions to calculate the euclidean length of a vector and therefore require significant overhead for data shuffling. The one advantage of this approach lies in the treatment of conditional code, e.g. in collision detections. As only one particle is processed at a time it is possible to skip unneeded computations.

Processing multiple particles by packing their respective x, y, and z coordinates into different register reverses these drawbacks and advantages. This treatment trivially allows to go from SSE to AVX instructions by simply loading twice as many particles into the registers. In contrast it is no longer possible to use conditional statements to skip portions of the code. A conditional check may be true for some particles and false for others. Instead we use the conditional check to produce a mask indicating for which particles the check holds and for which it doesn't. We then run the conditional code for all particles and mask the application of the results according to this indication mask.

\mypar{JIT Compiler}
The JIT compiler exclusively uses the multiple particle approach for vectorization which allows the intermediate (SSA-IR) code to only deal with the scalar form of the rules that are then simply executed on 1, 4 or 8 particles at a time. This means vectorization exclusively happens in final code generation and is of no concern for the optimization passes implemented on the SSA-IR form.

The rules are initially given as strings in a strongly reduced C style syntax. For example the euler update rule looks as follows:
\begin{lstlisting}
dt = [8]
[0] = [0] + dt*[4]
[1] = [1] + dt*[5]
[2] = [2] + dt*[6]
\end{lstlisting}
The numbers in brackets are indexing into implicit arrays that have their meaning assigned by the code generator. In our case indices 0-7 refer to particle components and higher indices refer to a constant array.

These rules are then translated to SSA-IR form by a simple single pass recursive descent parser. The SSA-IR form is stored as an array of fixed width 64bit instructions which contain an opcode, flags and up to three operands. The operands are indices that either refer to previous instructions in the SSA-IR array or to the input arrays in case of the load and store instructions. Using this flat and fixed width representation allows efficient implementation of optimizations passes.

\mypar{Optimization}
The first optimization pass consists of removing redundant stores and loads. Since concatenating rules will result in multiple redundant loads (almost every rule will load position for example) and stores (only the last store to any component will actually have an effect on the memory), a significant amount of those operations can be combined or removed. Similarily redundant computations which show up as identical instructions in the SSA-IR can also be skipped. Both of these operations are done by updating a index remapping array. All uses of redundant instrunctions are remapped to use the first instance of that instruction. As a result the additional instances are not referred to anymore and leaves them as dead code.
In the next step instructions are recursively marked as being live starting from the remaining store instructions. The live instruction can then be compacted again removing all the dead code including the redundant operations from the previous steps. Depending on the rule combination this process can remove up to half the instructions of the original rules. It does not however reduce the length of the critical path though.
The last step before code generation is the scheduler. The scheduler tries to heuristically reorder the instructions into a better sequence. Since the processors we are targeting have reasonably large reorder buffers (168 slots for Intel Ivy Bridge microarchitecture) the exact ordering between single instructions is not that important. Still we want largely independent paths of execution to be interleaved in the greater scheme since the loop bodies as a whole easily exceed the reorder buffers size. Additionally the live ranges of values can be changed by rescheduling which allows the scheduler to influence the register pressure the code generator will have to deal with.
The scheduler therefore operates by enumerating all eligible instructions that have their dependencies fulfilled and assigns a score to them. It then schedules the instruction with the highest score and repeats the process until all instructions are scheduled. The scheduler as implemented uses two factors that influence the score: criticality and influence on live registers. Criticality is the sum of latencies of dependent instructions. Influence on live registers is the change in amount of live registers scheduling the instruction results in. The scoring therefore prefers instructions that have high criticality and reduce the amount of live registers.

\mypar{Code Generation}
Each of the three vectorization levels (scalar, 4-wide, 8-wide) have their own code generator. The main difference between the versions is how they translate load and store instructions since the non-scalar versions need to gather the particle components from memory into registers. Instruction selection is done by a simple lookup table since the SSA-IR instructions have already been chosen to directly correspond to the underlying x86-64 instruction set. The main challenge therefore is the allocation and spilling of registers. To use the instruction sets ability to directly use memory operands for arithmetic instructions load operations are deferred to the first usage of their result. When spilling is neccessary the register that has its next use the farthest from the current position is assigned a free slot on the stack and moved there. In addition to the translation of the block itself the code generator also generates some fixed code such as the loop structure and function boilerplate around the optimized rules to make them a callable function following the system-V AMD64 ABI. This way the resulting buffer can simply be cast to an appropriate function type and called from the C code. The code generator allocates memory using the \texttt{mmap} system call to obtain page aligned memory with read and write permissions. Before returning that memory is changed to read and execute permissions using the \texttt{mprotect} system call. This is necessary since memory obtained from \texttt{malloc} will typically not have execute permissions and calling into it would therefore result in segmentation faults.
%~ For this class, explain all the optimizations you performed. This mean, you first very briefly
%~ explain the baseline implementation, then go through locality and other optimizations, and finally SSE (every project will be slightly different of course). Show or mention relevant analysis or assumptions. A few examples: 1) Profiling may lead you to optimize one part first; 2) bandwidth plus data transfer analysis may show that it is memory bound; 3) it may be too hard to implement the algorithm in full generality: make assumptions and state them (e.g., we assume $n$ is divisible by 4; or, we consider only one type of input image); 4) explain how certain data accesses have poor locality. Generally, any type of analysis adds value to your work.
%~
%~ As important as the final results is to show that you took a structured, organized approach to the optimization and that you explain why you did what you did.
%~
%~ Mention and cite any external resources including library or other code.
%~
%~ Good visuals or even brief code snippets to illustrate what you did are good. Pasting large amounts of code to fill the space is not good.
